{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d92f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,BaseMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817d5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemini-2.5-flash\"  # or another available model\n",
    "llm=ChatGoogleGenerativeAI(model=model_name, api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9995f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jokestate(TypedDict):\n",
    "  topic:str\n",
    "  joke:str\n",
    "  explaintion:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c639ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(state:jokestate):\n",
    "  propmt=f\" generate a joke on the topic {state['topic']}\"\n",
    "  response=llm.invoke(propmt).content\n",
    "  return {\"joke\":response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b31a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explaintion(state:jokestate):\n",
    "  propmt=f\" write an explaination for the joke : {state['joke']}\"\n",
    "  response=llm.invoke(propmt).content\n",
    "  return {\"explaintion\":response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7577a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=StateGraph(jokestate)\n",
    "graph.add_node(\"generate_joke\",generate_joke)\n",
    "graph.add_node(\"generate_explaintion\",generate_explaintion)\n",
    "\n",
    "graph.add_edge(START,\"generate_joke\")\n",
    "graph.add_edge(\"generate_joke\",\"generate_explaintion\")\n",
    "graph.add_edge(\"generate_explaintion\",END)\n",
    "\n",
    "checkpointer=InMemorySaver()\n",
    "\n",
    "workflow=graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "937268f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'technology',\n",
       " 'joke': 'Why did the computer go to the doctor?\\n\\nBecause it had a virus, and all its files were feeling a bit *compressed*!',\n",
       " 'explaintion': 'This joke works by playing on two puns and the concept of personification:\\n\\n1.  **The \"Virus\" Pun:**\\n    *   In the human world, a \"virus\" is a germ or illness that makes people sick, requiring a doctor\\'s visit.\\n    *   In the computer world, a \"virus\" is malicious software that infects a computer system, causing it to malfunction or crash.\\n    *   The humor comes from applying the human reason for going to the doctor (having a biological virus) to a computer, which would have a digital virus.\\n\\n2.  **The \"Compressed\" Pun:**\\n    *   For people, feeling \"compressed\" can mean feeling stressed, pressured, stifled, or tightly squeezed – an emotional or physical state someone might seek help for.\\n    *   For computers, \"compressing\" files is a technical process where the size of files is reduced (like creating a ZIP file) to save space or make them easier to transfer.\\n    *   The joke personifies the computer\\'s files by giving them human feelings of being \"compressed,\" as if they\\'re stressed or anxious because they\\'ve undergone a digital compression process.\\n\\n**In essence, the joke is funny because:**\\n\\nIt treats a computer as if it were a person with human problems (\"virus\" as an illness, \"files feeling compressed\" as emotional stress), using technical computer terms in a way that suggests human-like ailments.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"thread_id\":\"joke_thread_1\"}}\n",
    "workflow.invoke({\"topic\":\"technology\"},config=config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65dba92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'technology', 'joke': 'Why did the computer go to the doctor?\\n\\nBecause it had a virus, and all its files were feeling a bit *compressed*!', 'explaintion': 'This joke works by playing on two puns and the concept of personification:\\n\\n1.  **The \"Virus\" Pun:**\\n    *   In the human world, a \"virus\" is a germ or illness that makes people sick, requiring a doctor\\'s visit.\\n    *   In the computer world, a \"virus\" is malicious software that infects a computer system, causing it to malfunction or crash.\\n    *   The humor comes from applying the human reason for going to the doctor (having a biological virus) to a computer, which would have a digital virus.\\n\\n2.  **The \"Compressed\" Pun:**\\n    *   For people, feeling \"compressed\" can mean feeling stressed, pressured, stifled, or tightly squeezed – an emotional or physical state someone might seek help for.\\n    *   For computers, \"compressing\" files is a technical process where the size of files is reduced (like creating a ZIP file) to save space or make them easier to transfer.\\n    *   The joke personifies the computer\\'s files by giving them human feelings of being \"compressed,\" as if they\\'re stressed or anxious because they\\'ve undergone a digital compression process.\\n\\n**In essence, the joke is funny because:**\\n\\nIt treats a computer as if it were a person with human problems (\"virus\" as an illness, \"files feeling compressed\" as emotional stress), using technical computer terms in a way that suggests human-like ailments.'}, next=(), config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc204-6db2-6952-8002-f2642e977bb3'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-12-18T14:46:03.431765+00:00', parent_config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc203-acc9-6483-8001-3a48d6e54461'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_state(config=config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ad0d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'topic': 'technology', 'joke': 'Why did the computer go to the doctor?\\n\\nBecause it had a virus, and all its files were feeling a bit *compressed*!', 'explaintion': 'This joke works by playing on two puns and the concept of personification:\\n\\n1.  **The \"Virus\" Pun:**\\n    *   In the human world, a \"virus\" is a germ or illness that makes people sick, requiring a doctor\\'s visit.\\n    *   In the computer world, a \"virus\" is malicious software that infects a computer system, causing it to malfunction or crash.\\n    *   The humor comes from applying the human reason for going to the doctor (having a biological virus) to a computer, which would have a digital virus.\\n\\n2.  **The \"Compressed\" Pun:**\\n    *   For people, feeling \"compressed\" can mean feeling stressed, pressured, stifled, or tightly squeezed – an emotional or physical state someone might seek help for.\\n    *   For computers, \"compressing\" files is a technical process where the size of files is reduced (like creating a ZIP file) to save space or make them easier to transfer.\\n    *   The joke personifies the computer\\'s files by giving them human feelings of being \"compressed,\" as if they\\'re stressed or anxious because they\\'ve undergone a digital compression process.\\n\\n**In essence, the joke is funny because:**\\n\\nIt treats a computer as if it were a person with human problems (\"virus\" as an illness, \"files feeling compressed\" as emotional stress), using technical computer terms in a way that suggests human-like ailments.'}, next=(), config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc204-6db2-6952-8002-f2642e977bb3'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-12-18T14:46:03.431765+00:00', parent_config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc203-acc9-6483-8001-3a48d6e54461'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'technology', 'joke': 'Why did the computer go to the doctor?\\n\\nBecause it had a virus, and all its files were feeling a bit *compressed*!'}, next=('generate_explaintion',), config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc203-acc9-6483-8001-3a48d6e54461'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-12-18T14:45:43.203545+00:00', parent_config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc201-d6b2-6c9e-8000-f7262c70f23d'}}, tasks=(PregelTask(id='9f64485c-a63f-984f-44eb-fd39675e31ee', name='generate_explaintion', path=('__pregel_pull', 'generate_explaintion'), error=None, interrupts=(), state=None, result={'explaintion': 'This joke works by playing on two puns and the concept of personification:\\n\\n1.  **The \"Virus\" Pun:**\\n    *   In the human world, a \"virus\" is a germ or illness that makes people sick, requiring a doctor\\'s visit.\\n    *   In the computer world, a \"virus\" is malicious software that infects a computer system, causing it to malfunction or crash.\\n    *   The humor comes from applying the human reason for going to the doctor (having a biological virus) to a computer, which would have a digital virus.\\n\\n2.  **The \"Compressed\" Pun:**\\n    *   For people, feeling \"compressed\" can mean feeling stressed, pressured, stifled, or tightly squeezed – an emotional or physical state someone might seek help for.\\n    *   For computers, \"compressing\" files is a technical process where the size of files is reduced (like creating a ZIP file) to save space or make them easier to transfer.\\n    *   The joke personifies the computer\\'s files by giving them human feelings of being \"compressed,\" as if they\\'re stressed or anxious because they\\'ve undergone a digital compression process.\\n\\n**In essence, the joke is funny because:**\\n\\nIt treats a computer as if it were a person with human problems (\"virus\" as an illness, \"files feeling compressed\" as emotional stress), using technical computer terms in a way that suggests human-like ailments.'}),), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'technology'}, next=('generate_joke',), config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc201-d6b2-6c9e-8000-f7262c70f23d'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-12-18T14:44:53.911260+00:00', parent_config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc201-d69c-6d94-bfff-29ca95df6804'}}, tasks=(PregelTask(id='6759ce89-d5ab-c869-5cfe-e96eddef83b8', name='generate_joke', path=('__pregel_pull', 'generate_joke'), error=None, interrupts=(), state=None, result={'joke': 'Why did the computer go to the doctor?\\n\\nBecause it had a virus, and all its files were feeling a bit *compressed*!'}),), interrupts=()),\n",
       " StateSnapshot(values={}, next=('__start__',), config={'configurable': {'thread_id': 'joke_thread_1', 'checkpoint_ns': '', 'checkpoint_id': '1f0dc201-d69c-6d94-bfff-29ca95df6804'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-12-18T14:44:53.902274+00:00', parent_config=None, tasks=(PregelTask(id='b0ba25b4-1bb2-f9f5-4a10-36ab3a104e6e', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'topic': 'technology'}),), interrupts=())]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(workflow.get_state_history(config=config1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2181fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 50.060930075s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2958\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\models.py:5218\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5217\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5218\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5222\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\models.py:4000\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3998\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4005\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4006\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 50.060930075s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m config={\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mjoke_thread_2\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscience\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mgenerate_joke\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_joke\u001b[39m(state:jokestate):\n\u001b[32m      2\u001b[39m   propmt=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m generate a joke on the topic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m   response=\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpropmt\u001b[49m\u001b[43m)\u001b[49m.content\n\u001b[32m      4\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mjoke\u001b[39m\u001b[33m\"\u001b[39m:response}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2461\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2458\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2459\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2962\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   2959\u001b[39m         **request,\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2962\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ACER\\OneDrive\\Documents\\GitHub\\Langraph_learning\\myenv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 50.060930075s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '50s'}]}}",
      "During task with name 'generate_joke' and id 'da1a72cb-185f-b0c1-5e61-35d23d90b2af'"
     ]
    }
   ],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"joke_thread_2\"}}\n",
    "workflow.invoke({\"topic\":\"science\"},config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe980f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_state(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "384eb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000012707C6DF10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4069b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
